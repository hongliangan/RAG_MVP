#!/usr/bin/env python3
"""
æµ‹è¯•å®Œæ•´çš„RAGæµç¨‹ï¼ŒåŒ…æ‹¬æ–°çš„æ–‡æœ¬åˆ‡ç‰‡åŠŸèƒ½
"""
import os
import sys
from rag_core.data_loader import load_documents
from rag_core.embedding import embed_documents
from rag_core.retriever import retrieve
from rag_core.llm_api import call_llm_api
from utils.config import get_text_chunk_config

def test_rag_with_different_chunk_configs():
    """æµ‹è¯•ä¸åŒåˆ‡ç‰‡é…ç½®ä¸‹çš„RAGæ•ˆæœ"""
    
    # æµ‹è¯•æ–‡æ¡£è·¯å¾„
    test_doc_path = "web/uploads/xRay.docx"
    
    if not os.path.exists(test_doc_path):
        print(f"âŒ æµ‹è¯•æ–‡æ¡£ä¸å­˜åœ¨: {test_doc_path}")
        return
    
    print("=== RAGæµç¨‹æµ‹è¯•ï¼ˆä¸åŒåˆ‡ç‰‡é…ç½®ï¼‰ ===\n")
    
    # æµ‹è¯•æŸ¥è¯¢
    test_query = "Xå°„çº¿æ˜¯ä»€ä¹ˆï¼Ÿå®ƒæœ‰ä»€ä¹ˆç”¨é€”ï¼Ÿ"
    print(f"æµ‹è¯•æŸ¥è¯¢: {test_query}\n")
    
    # é…ç½®1: æŒ‰æ®µè½åˆ†å‰²ï¼ˆé»˜è®¤ï¼‰
    print("1. æŒ‰æ®µè½åˆ†å‰²ï¼ˆé»˜è®¤é…ç½®ï¼‰:")
    config1 = {"split_method": "paragraph"}
    test_rag_flow(test_doc_path, test_query, config1, "æ®µè½åˆ†å‰²")
    
    # é…ç½®2: æŒ‰å­—ç¬¦æ•°åˆ†å‰²
    print("\n2. æŒ‰å­—ç¬¦æ•°åˆ†å‰²ï¼ˆchunk_size=500ï¼‰:")
    config2 = {
        "split_method": "character",
        "chunk_size": 500,
        "chunk_overlap": 100
    }
    test_rag_flow(test_doc_path, test_query, config2, "å­—ç¬¦åˆ†å‰²")
    
    # é…ç½®3: æŒ‰å¥å­åˆ†å‰²
    print("\n3. æŒ‰å¥å­åˆ†å‰²ï¼ˆmax_sentences=3ï¼‰:")
    config3 = {
        "split_method": "sentence",
        "max_sentences_per_chunk": 3
    }
    test_rag_flow(test_doc_path, test_query, config3, "å¥å­åˆ†å‰²")

def test_rag_flow(doc_path, query, chunk_config, config_name):
    """æµ‹è¯•å•ä¸ªRAGæµç¨‹"""
    
    try:
        print(f"  é…ç½®: {config_name}")
        
        # 1. æ–‡æ¡£åŠ è½½å’Œåˆ‡ç‰‡
        print("  ğŸ“„ åŠ è½½æ–‡æ¡£å¹¶åˆ‡ç‰‡...")
        chunks = load_documents(doc_path, chunk_config)
        print(f"    åˆ‡ç‰‡ç»“æœ: {len(chunks)} ä¸ªå—")
        for i, chunk in enumerate(chunks[:2], 1):
            print(f"    å— {i}: {len(chunk)} å­—ç¬¦ - {chunk[:60]}...")
        if len(chunks) > 2:
            print(f"    ... è¿˜æœ‰ {len(chunks)-2} ä¸ªå—")
        
        # 2. å‘é‡åŒ–
        print("  ğŸ”¢ ç”Ÿæˆå‘é‡...")
        embeddings = embed_documents(chunks)
        print(f"    å‘é‡åŒ–å®Œæˆ: {len(embeddings)} ä¸ªå‘é‡")
        
        # 3. æ£€ç´¢
        print("  ğŸ” æ£€ç´¢ç›¸å…³ç‰‡æ®µ...")
        similar_chunks = retrieve(query, embeddings, chunks, top_k=3)
        print(f"    æ£€ç´¢åˆ° {len(similar_chunks)} ä¸ªç›¸å…³ç‰‡æ®µ")
        for i, chunk in enumerate(similar_chunks, 1):
            print(f"    ç›¸å…³ç‰‡æ®µ {i}: {chunk[:80]}...")
        
        # 4. LLMç”Ÿæˆ
        print("  ğŸ¤– ç”Ÿæˆå›ç­”...")
        context = "\n\n".join(similar_chunks)
        prompt = f"""åŸºäºä»¥ä¸‹ä¸Šä¸‹æ–‡ä¿¡æ¯å›ç­”é—®é¢˜ï¼š

ä¸Šä¸‹æ–‡ï¼š
{context}

é—®é¢˜ï¼š{query}

è¯·æä¾›å‡†ç¡®ã€è¯¦ç»†çš„å›ç­”ï¼š"""
        
        response = call_llm_api(prompt)
        print(f"    LLMå›ç­”: {response[:200]}...")
        
        print("  âœ… æµç¨‹å®Œæˆ")
        
    except Exception as e:
        print(f"  âŒ é”™è¯¯: {e}")

def test_chunk_config_comparison():
    """æ¯”è¾ƒä¸åŒåˆ‡ç‰‡é…ç½®çš„æ•ˆæœ"""
    
    print("\n=== åˆ‡ç‰‡é…ç½®æ•ˆæœæ¯”è¾ƒ ===\n")
    
    # æµ‹è¯•æ–‡æœ¬
    test_text = """
    Xå°„çº¿æ˜¯ä¸€ç§ç”µç£è¾å°„ï¼Œæ³¢é•¿æ¯”å¯è§å…‰çŸ­ï¼Œèƒ½é‡æ¯”å¯è§å…‰é«˜ã€‚Xå°„çº¿å…·æœ‰ç©¿é€æ€§ï¼Œèƒ½å¤Ÿç©¿é€äººä½“è½¯ç»„ç»‡ï¼Œä½†ä¼šè¢«éª¨éª¼ç­‰å¯†åº¦è¾ƒå¤§çš„ç»„ç»‡å¸æ”¶ã€‚

    Xå°„çº¿åœ¨åŒ»å­¦è¯Šæ–­ä¸­å¹¿æ³›åº”ç”¨ï¼Œä¸»è¦ç”¨äºæ£€æŸ¥éª¨æŠ˜ã€è‚ºéƒ¨ç–¾ç—…ã€ç‰™é½¿é—®é¢˜ç­‰ã€‚é€šè¿‡Xå°„çº¿æ‘„å½±ï¼ŒåŒ»ç”Ÿå¯ä»¥è§‚å¯Ÿåˆ°äººä½“å†…éƒ¨çš„ç»“æ„ï¼Œå¸®åŠ©è¯Šæ–­ç–¾ç—…ã€‚

    Xå°„çº¿æŠ€æœ¯ä¹Ÿåœ¨å·¥ä¸šæ£€æµ‹ã€å®‰å…¨æ£€æŸ¥ã€ææ–™åˆ†æç­‰é¢†åŸŸå‘æŒ¥é‡è¦ä½œç”¨ã€‚åœ¨æœºåœºå®‰æ£€ä¸­ï¼ŒXå°„çº¿æœºå¯ä»¥æ£€æµ‹è¡Œæä¸­çš„è¿ç¦ç‰©å“ã€‚

    ç„¶è€Œï¼ŒXå°„çº¿å¯¹äººä½“æœ‰ä¸€å®šçš„è¾å°„å±å®³ï¼Œå› æ­¤åœ¨ä½¿ç”¨æ—¶éœ€è¦ä¸¥æ ¼æ§åˆ¶å‰‚é‡ï¼Œå¹¶é‡‡å–å¿…è¦çš„é˜²æŠ¤æªæ–½ã€‚
    """
    
    # ä¸åŒé…ç½®
    configs = {
        "æ®µè½åˆ†å‰²": {"split_method": "paragraph"},
        "å­—ç¬¦åˆ†å‰²(500)": {"split_method": "character", "chunk_size": 500, "chunk_overlap": 100},
        "å­—ç¬¦åˆ†å‰²(300)": {"split_method": "character", "chunk_size": 300, "chunk_overlap": 50},
        "å¥å­åˆ†å‰²": {"split_method": "sentence", "max_sentences_per_chunk": 2}
    }
    
    from rag_core.text_splitter import TextSplitter
    
    for name, config in configs.items():
        print(f"{name}:")
        splitter = TextSplitter(config)
        chunks = splitter.split_text(test_text)
        print(f"  å—æ•°: {len(chunks)}")
        print(f"  å¹³å‡é•¿åº¦: {sum(len(c) for c in chunks) // len(chunks) if chunks else 0} å­—ç¬¦")
        print(f"  é•¿åº¦èŒƒå›´: {min(len(c) for c in chunks) if chunks else 0} - {max(len(c) for c in chunks) if chunks else 0} å­—ç¬¦")
        print()

if __name__ == "__main__":
    # æµ‹è¯•åˆ‡ç‰‡é…ç½®æ¯”è¾ƒ
    test_chunk_config_comparison()
    
    # æµ‹è¯•å®Œæ•´RAGæµç¨‹
    test_rag_with_different_chunk_configs() 